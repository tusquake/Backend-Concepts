# Rate Limiter Algorithms - Complete Guide

## ğŸ“– Introduction

Rate limiting is a technique used to control the rate of requests a user or service can make to an API or system within a specified time window. It's essential for:

- **Preventing abuse** and DDoS attacks
- **Ensuring fair resource usage** among users
- **Maintaining system stability** under high load
- **Protecting backend services** from overload
- **Managing costs** for paid APIs

---

## ğŸ”¹ 1. Fixed Window Counter

### ğŸ§  How It Works

Time is divided into **fixed windows** of a specific duration (e.g., 1 second, 1 minute). Each user has a counter that tracks requests made in the current window.

**Algorithm Steps:**
1. Check current time window (e.g., current second)
2. Get request count for this user in current window
3. If `count < limit` â†’ Allow request, increment counter
4. If `count >= limit` â†’ Block request
5. When window ends â†’ Reset counter to 0

### âœ… Example

**Limit:** 3 requests per second

**Timeline:**
```
12:00:05.100 â†’ Request 1 âœ… (count: 1)
12:00:05.200 â†’ Request 2 âœ… (count: 2)
12:00:05.400 â†’ Request 3 âœ… (count: 3)
12:00:05.500 â†’ Request 4 âŒ (limit exceeded)
12:00:06.000 â†’ Counter resets (count: 0)
12:00:06.100 â†’ Request 5 âœ… (count: 1)
```

### ğŸŒ Real-World Analogy

**Restaurant Buffet System:**
- You're allowed **3 plates per minute**
- Try to grab a 4th plate in the same minute â†’ Waiter blocks you
- When the next minute starts â†’ Your plate counter resets to 0

### âš–ï¸ Pros & Cons

**Pros:**
- âœ… Simple to implement and understand
- âœ… Memory efficient (only stores counter per user)
- âœ… Fast performance (O(1) operations)
- âœ… Easy to scale with distributed counters (Redis)

**Cons:**
- âŒ **Burst problem at window boundaries**
  - Example: 3 requests at 12:00:00.999, 3 more at 12:00:01.001 = 6 requests in 2ms
- âŒ Unfair for users making requests near window boundaries
- âŒ Can cause "thundering herd" at window reset

### ğŸ’» Use Cases

- Simple APIs with moderate traffic
- When approximate rate limiting is acceptable
- Systems prioritizing performance over precision
- Internal microservices rate limiting

---

## ğŸ”¹ 2. Sliding Window Log

### ğŸ§  How It Works

Maintains a **log of timestamps** for all requests. For each new request, the algorithm checks how many requests occurred in the past time window.

**Algorithm Steps:**
1. Record timestamp of new request
2. Remove all timestamps older than `(current_time - window_size)`
3. Count remaining timestamps in the log
4. If `count < limit` â†’ Allow request, add timestamp to log
5. If `count >= limit` â†’ Block request

### âœ… Example

**Limit:** 3 requests per second (1000ms window)

**Timeline:**
```
12:00:05.100 â†’ Request 1 âœ… (log: [100])
12:00:05.200 â†’ Request 2 âœ… (log: [100, 200])
12:00:05.400 â†’ Request 3 âœ… (log: [100, 200, 400])
12:00:05.900 â†’ Request 4 âŒ (log still has 4 requests)
12:00:06.200 â†’ Request 5 âœ… (timestamp 100 expired, log: [200, 400, 900, 1200])
```

### ğŸŒ Real-World Analogy

**Office Coffee Machine:**
- Allows **3 cups per rolling minute**
- Coffee at `9:00:05`, `9:00:15`, `9:00:25`
- At `9:00:50` â†’ Machine says "limit reached"
- At `9:01:06` â†’ Your `9:00:05` coffee no longer counts, you can get another

### âš–ï¸ Pros & Cons

**Pros:**
- âœ… No burst problem - smooth and fair rate limiting
- âœ… Accurate tracking of requests
- âœ… Works perfectly for sliding time windows
- âœ… Fair for all users regardless of timing

**Cons:**
- âŒ High memory usage (stores all timestamps)
- âŒ Slower than fixed window (requires log cleanup)
- âŒ Can be expensive at scale (many users Ã— many requests)
- âŒ Requires sorted data structure for efficiency

### ğŸ’» Use Cases

- High-value APIs requiring precise rate limiting
- Premium/paid API tiers
- Security-critical systems
- When fairness is more important than performance

---

## ğŸ”¹ 3. Token Bucket

### ğŸ§  How It Works

Each user has a **bucket of tokens** that refills at a constant rate. Each request consumes one token. When the bucket is empty, requests are rejected.

**Algorithm Steps:**
1. Calculate tokens to add: `(current_time - last_refill_time) Ã— refill_rate`
2. Add tokens to bucket (max = bucket capacity)
3. If `bucket >= 1` â†’ Allow request, consume 1 token
4. If `bucket < 1` â†’ Block request
5. Update `last_refill_time`

### âœ… Example

**Configuration:**
- Bucket capacity: 5 tokens
- Refill rate: 2 tokens/second

**Timeline:**
```
Time 0.0s â†’ Bucket: 5 tokens
  Request 1 âœ… â†’ Bucket: 4 tokens
  Request 2 âœ… â†’ Bucket: 3 tokens
  Request 3 âœ… â†’ Bucket: 2 tokens
  
Time 0.5s â†’ Refill: +1 token â†’ Bucket: 3 tokens
  Request 4 âœ… â†’ Bucket: 2 tokens
  Request 5 âœ… â†’ Bucket: 1 token
  Request 6 âœ… â†’ Bucket: 0 tokens
  Request 7 âŒ â†’ Bucket: 0 tokens (blocked)
  
Time 1.0s â†’ Refill: +1 token â†’ Bucket: 1 token
  Request 8 âœ… â†’ Bucket: 0 tokens
```

### ğŸŒ Real-World Analogy

**Parking Lot:**
- Parking lot has **5 spaces** (bucket capacity)
- Each car needs **1 space** (1 token per request)
- Every 30 minutes, **1 car leaves** (token refill)
- When lot is full â†’ New cars must wait
- Ensures parking never gets overwhelmed

### âš–ï¸ Pros & Cons

**Pros:**
- âœ… Handles burst traffic elegantly (up to bucket size)
- âœ… Memory efficient (only stores bucket state)
- âœ… Flexible - balances bursts and sustained rate
- âœ… Industry standard (used by AWS, Stripe, etc.)

**Cons:**
- âŒ More complex than fixed window
- âŒ Requires careful tuning of bucket size and refill rate
- âŒ Can be difficult to reason about for users
- âŒ Potential for race conditions in distributed systems

### ğŸ’» Use Cases

- Cloud APIs (AWS API Gateway uses this)
- Payment processing APIs (Stripe)
- Systems needing burst tolerance
- When smooth traffic shaping is desired

---

## ğŸ”¹ 4. Leaky Bucket

### ğŸ§  How It Works

Similar to token bucket, but focuses on **outgoing traffic rate**. Requests are queued and processed at a fixed rate, like water leaking from a bucket.

**Algorithm Steps:**
1. Add incoming request to queue
2. Process requests from queue at fixed rate
3. If queue is full â†’ Reject request
4. Ensures output rate is constant

### âœ… Example

**Configuration:**
- Queue size: 5 requests
- Process rate: 1 request/second

**Timeline:**
```
Time 0.0s â†’ 5 requests arrive â†’ Queue: [R1, R2, R3, R4, R5]
Time 0.0s â†’ 6th request âŒ (queue full)
Time 1.0s â†’ Process R1 âœ… â†’ Queue: [R2, R3, R4, R5]
Time 1.0s â†’ New request arrives âœ… â†’ Queue: [R2, R3, R4, R5, R6]
Time 2.0s â†’ Process R2 âœ… â†’ Queue: [R3, R4, R5, R6]
```

### âš–ï¸ Pros & Cons

**Pros:**
- âœ… Smooth output rate
- âœ… Good for network traffic shaping
- âœ… Prevents system overload

**Cons:**
- âŒ Adds latency (queuing delay)
- âŒ Queue management overhead
- âŒ Not suitable for real-time APIs

### ğŸ’» Use Cases

- Network traffic shaping
- Video streaming rate control
- Background job processing

---

## ğŸ”¹ 5. Sliding Window Counter (Hybrid)

### ğŸ§  How It Works

Combines **Fixed Window** and **Sliding Window Log** for better accuracy without high memory cost.

**Formula:**
```
Rate = (prev_window_count Ã— overlap_percentage) + current_window_count
```

### âœ… Example

**Limit:** 10 requests per minute

**Scenario at 12:00:30 (30 seconds into current minute):**
- Previous window (11:59): 8 requests
- Current window (12:00): 4 requests
- Overlap: 50% (30 seconds)

**Calculation:**
```
Rate = (8 Ã— 0.5) + 4 = 4 + 4 = 8 requests
Result: âœ… Allow (under limit of 10)
```

### âš–ï¸ Pros & Cons

**Pros:**
- âœ… Better than fixed window (no burst problem)
- âœ… More memory efficient than sliding log
- âœ… Good balance of accuracy and performance

**Cons:**
- âŒ Still approximate (not as accurate as sliding log)
- âŒ More complex implementation

### ğŸ’» Use Cases

- High-traffic APIs needing good accuracy
- Systems with memory constraints
- When precise sliding window is too expensive

---

## ğŸ“Š Algorithm Comparison

| Algorithm | Memory | Accuracy | Performance | Burst Handling | Complexity |
|-----------|--------|----------|-------------|----------------|------------|
| **Fixed Window** | Low | Moderate | Excellent | Poor | Simple |
| **Sliding Log** | High | Excellent | Good | Excellent | Moderate |
| **Token Bucket** | Low | Good | Excellent | Excellent | Moderate |
| **Leaky Bucket** | Moderate | Excellent | Good | Poor | Moderate |
| **Sliding Counter** | Low | Good | Excellent | Good | Moderate |

---

## ğŸ¯ Choosing the Right Algorithm

### Use **Fixed Window** when:
- Simple internal APIs
- Performance is critical
- Approximate limiting is acceptable
- Low resource usage is priority

### Use **Sliding Window Log** when:
- Precision is critical
- Premium/paid API tiers
- Security-sensitive operations
- Fairness is more important than cost

### Use **Token Bucket** when:
- Need to allow bursts
- Industry-standard behavior expected
- Cloud/payment APIs
- Balance between flexibility and control

### Use **Leaky Bucket** when:
- Need constant output rate
- Network traffic shaping
- Background job processing

### Use **Sliding Window Counter** when:
- Need better accuracy than fixed window
- Memory is constrained
- High traffic volume

---

## ğŸ› ï¸ Implementation Considerations

### Distributed Systems

**Challenges:**
- Race conditions across multiple servers
- Clock synchronization issues
- Network latency

**Solutions:**
- Use centralized storage (Redis, Memcached)
- Implement distributed locks
- Use Lua scripts for atomic operations
- Consider eventual consistency trade-offs

### Storage Options

| Storage | Pros | Cons |
|---------|------|------|
| **Redis** | Fast, atomic operations, TTL support | Single point of failure (without cluster) |
| **Memory** | Fastest, no network overhead | Lost on restart, not distributed |
| **Database** | Persistent, scalable | Slower, more overhead |

### Key Design Decisions

1. **Granularity:** Per user? Per IP? Per API key?
2. **Scope:** Global? Per endpoint? Per resource?
3. **Response:** Block (429 error)? Queue? Throttle?
4. **Headers:** Return rate limit info to clients?
5. **Bypass:** Allow whitelist for critical users?

---

## ğŸŒ Industry Examples

- **Twitter API:** Token bucket (15 requests per 15-min window)
- **GitHub API:** Sliding window (5000 requests per hour)
- **Stripe API:** Token bucket with different rates per endpoint
- **AWS API Gateway:** Token bucket (burst + sustained rate)
- **Cloudflare:** Multiple algorithms based on plan

---

## ğŸ’¡ Summary

Rate limiting is essential for building robust, scalable APIs. The choice of algorithm depends on your specific requirements:

- **Simple & fast?** â†’ Fixed Window
- **Precise & fair?** â†’ Sliding Window Log
- **Flexible with bursts?** â†’ Token Bucket
- **Smooth output?** â†’ Leaky Bucket
- **Balanced approach?** â†’ Sliding Window Counter

Most production systems use **Token Bucket** or **Sliding Window Counter** as they provide the best balance of accuracy, performance, and user experience.